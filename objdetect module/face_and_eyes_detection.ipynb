{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0d2aeef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#objdetect module of opencv .\n",
    "#this consists of cascade classifier ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57496fb0",
   "metadata": {},
   "source": [
    "'''\n",
    "object detection by cascade calassifers is proposed by viola and jone. It is also called viola and jones algorithms ,\n",
    "which is used to deetect faces and eyes . In This process cascade is trained by  a lot of positive and negative images .\n",
    "and helps to detection.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066d7c87",
   "metadata": {},
   "source": [
    "The process is not as simple as this. Each image is given an equal weight in the beginning. After each classification, weights of misclassified images are increased. Then the same process is done. New error rates are calculated. Also new weights. The process is continued until the required accuracy or error rate is achieved or the required number of features are found.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8135cb4e",
   "metadata": {},
   "source": [
    " Now, all possible sizes and locations of each kernel are used to calculate lots of features. (Just imagine how much computation it needs? Even a 24x24 window results over 160000 features). For each feature calculation, we need to find the sum of the pixels under white and black rectangles. To solve this, they introduced the integral image. However large your image, it reduces the calculations for a given pixel to an operation involving just four pixels. Nice, isn't it? It makes things super-fast.\n",
    "\n",
    "But among all these features we calculated, most of them are irrelevant. For example, consider the image below. The top row shows two good features. The first feature selected seems to focus on the property that the region of the eyes is often darker than the region of the nose and cheeks. The second feature selected relies on the property that the eyes are darker than the bridge of the nose. But the same windows applied to cheeks or any other place is irrelevant. So how do we select the best features out of 160000+ features? It is achieved by Adaboost.\n",
    "\n",
    "The final classifier is a weighted sum of these weak classifiers. It is called weak because it alone can't classify the image, but together with others forms a strong classifier. The paper says even 200 features provide detection with 95% accuracy. Their final setup had around 6000 features. (Imagine a reduction from 160000+ features to 6000 features. That is a big gain).\n",
    "\n",
    "For this they introduced the concept of Cascade of Classifiers. Instead of applying all 6000 features on a window, the features are grouped into different stages of classifiers and applied one-by-one. (Normally the first few stages will contain very many fewer features). If a window fails the first stage, discard it. We don't consider the remaining features on it. If it passes, apply the second stage of features and continue the process. The window which passes all stages is a face region. How is that plan!\n",
    "\n",
    "The authors' detector had 6000+ features with 38 stages with 1, 10, 25, 25 and 50 features in the first five stages. (The two features in the above image are actually obtained as the best two features from Adaboost). According to the authors, on average 10 features out of 6000+ are evaluated per sub-window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5eca912",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import cv2 as cv\n",
    "import argparse\n",
    "def detectAndDisplay(frame):\n",
    "    frame_gray = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)\n",
    "    frame_gray = cv.equalizeHist(frame_gray)\n",
    "    #-- Detect faces\n",
    "    faces = face_cascade.detectMultiScale(frame_gray)\n",
    "    for (x,y,w,h) in faces:\n",
    "        center = (x + w//2, y + h//2)\n",
    "        frame = cv.ellipse(frame, center, (w//2, h//2), 0, 0, 360, (255, 0, 255), 4)\n",
    "        faceROI = frame_gray[y:y+h,x:x+w]\n",
    "        #-- In each face, detect eyes\n",
    "        eyes = eyes_cascade.detectMultiScale(faceROI)\n",
    "        for (x2,y2,w2,h2) in eyes:\n",
    "            eye_center = (x + x2 + w2//2, y + y2 + h2//2)\n",
    "            radius = int(round((w2 + h2)*0.25))\n",
    "            frame = cv.circle(frame, eye_center, radius, (255, 0, 0 ), 4)\n",
    "    cv.imshow('Capture - Face detection', frame)\n",
    "# parser = argparse.ArgumentParser(description='Code for Cascade Classifier tutorial.')\n",
    "# parser.add_argument('--face_cascade', help='Path to face cascade.', default='C:/Users/madha/Desktop/opencv/sources/data/haarcascades/haarcascade_frontalface_alt.xml')\n",
    "# parser.add_argument('--eyes_cascade', help='Path to eyes cascade.', default='C:/Users/madha/Desktop/opencv/sources/data/haarcascades/haarcascade_eye_tree_eyeglasses.xml')\n",
    "# parser.add_argument('--camera', help='Camera divide number.', type=int, default=0)\n",
    "# args = parser.parse_args()\n",
    "# face_cascade_name = args.face_cascade\n",
    "# eyes_cascade_name = args.eyes_cascade\n",
    "face_cascade_name = 'C:/Users/madha/Desktop/opencv/sources/data/haarcascades/haarcascade_frontalface_alt.xml'\n",
    "eyes_cascade_name = 'C:/Users/madha/Desktop/opencv/sources/data/haarcascades/haarcascade_eye_tree_eyeglasses.xml'\n",
    "face_cascade = cv.CascadeClassifier()\n",
    "eyes_cascade = cv.CascadeClassifier()\n",
    "#-- 1. Load the cascades\n",
    "if not face_cascade.load(cv.samples.findFile(face_cascade_name)):\n",
    "    print('--(!)Error loading face cascade')\n",
    "    exit(0)\n",
    "if not eyes_cascade.load(cv.samples.findFile(eyes_cascade_name)):\n",
    "    print('--(!)Error loading eyes cascade')\n",
    "    exit(0)\n",
    "camera_device = 0\n",
    "#-- 2. Read the video stream\n",
    "cap = cv.VideoCapture(camera_device)\n",
    "if not cap.isOpened:\n",
    "    print('--(!)Error opening video capture')\n",
    "    exit(0)\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if frame is None:\n",
    "        print('--(!) No captured frame -- Break!')\n",
    "        break\n",
    "    detectAndDisplay(frame)\n",
    "    if cv.waitKey(10) == 27:\n",
    "        break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea56b3b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
